{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chaitanya-iitk/Judge_It_Well/blob/main/WEEK1/Week1_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Text Preprocessing**"
      ],
      "metadata": {
        "id": "IYACLCwsyxfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Data-** given in the below code cell\n",
        "\n",
        "**1.1: Preprocessing From Scratch**\n",
        "\n",
        "**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n",
        "\n",
        "1. Lowercasing: Convert text to lowercase.\n",
        "\n",
        "2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n",
        "\n",
        "3. Tokenization: Split the string into a list of words based on whitespace.\n",
        "\n",
        "4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n",
        "\n",
        "5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n",
        "\n",
        "\n",
        "Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n",
        "\n",
        "**Task:** Run this function on the first sentence of the corpus and print the result."
      ],
      "metadata": {
        "id": "MTP8EqylwqDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "qIRv3qS2bTFt"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import re\n",
        "def clean_text_scratch(sentences):\n",
        "  sentences=sentences.lower()\n",
        "  tokens= sentences.split()\n",
        "  stopwords= ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or']\n",
        "  for word in tokens:\n",
        "    for words in stopwords:\n",
        "      if word==words:\n",
        "        tokens.remove(word)\n",
        "  clean=[]\n",
        "  for word in tokens:\n",
        "    if word.endswith(\"ing\"):\n",
        "      clean.append(word[:-3])\n",
        "    elif word.endswith(\"s\"):\n",
        "      clean.append(word[:-1])\n",
        "    elif word.endswith(\"ly\"):\n",
        "      clean.append(word[:-2])\n",
        "    elif word.endswith(\"ed\"):\n",
        "      clean.append(word[:-2])\n",
        "    elif word.endswith(\";\"):\n",
        "      clean.append(word[:-1])\n",
        "    elif word.endswith(\",\"):\n",
        "      clean.append(word[:-1])\n",
        "    elif word.endswith(\".\"):\n",
        "      clean.append(word[:-1])\n",
        "    elif word.endswith(\"!\"):\n",
        "      clean.append(word[:-1])\n",
        "    elif word.endswith(\"?\"):\n",
        "      clean.append(word[:-1])\n",
        "    elif word.endswith(\"..\"):\n",
        "      clean.append(word[:-1])\n",
        "    else:\n",
        "      clean.append(word)\n",
        "  return clean\n",
        "\n",
        "print(clean_text_scratch(corpus[0]))\n"
      ],
      "metadata": {
        "id": "oR4BKqITy17z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498e8805-bee9-4fad-c37d-ca2f7a604da3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2: Preprocessing Using Tools**\n",
        "\n",
        "**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Use nltk.tokenize.word_tokenize.\n",
        "2. Use nltk.corpus.stopwords.\n",
        "3. Use nltk.stem.WordNetLemmatizer\n",
        "\n",
        "to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n",
        "\n",
        "\n",
        "**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."
      ],
      "metadata": {
        "id": "dN9rNq7WycqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def clean_text_nltk(text):\n",
        "  tokens= word_tokenize(text)\n",
        "  x= stopwords.words('english')\n",
        "  for word in tokens:\n",
        "    for words in x:\n",
        "      if word==words:\n",
        "        tokens.remove(word)\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  return tokens\n",
        "\n",
        "\n",
        "print(clean_text_nltk(cleaned_corpus[2]))"
      ],
      "metadata": {
        "id": "v_4FjuCqy5Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12e2149f-2223-451e-9992-9e02df592647"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
            "['The', 'quick', 'brown', 'fox', 'jump', 'the', 'lazy', 'dog']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "cleaned_corpus = [re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", doc) for doc in corpus]"
      ],
      "metadata": {
        "id": "yHH25TjHzuo5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Text Representation**"
      ],
      "metadata": {
        "id": "hPMrwva2y1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1: Bag of Words (BoW)**\n",
        "\n",
        "**Logic:**\n",
        "\n",
        "**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n",
        "\n",
        "**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n",
        "\n",
        "**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "cKa8NnZ5zLlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# NLTK downloads (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "arr=[]\n",
        "for i in range (0, len(corpus)):\n",
        "  arr.append(clean_text_scratch(cleaned_corpus[i]))\n",
        "\n",
        "join=[]\n",
        "for i in range (0,len(arr)):\n",
        "  join = join + arr[i]\n",
        "vocabulary=sorted(set(join))\n",
        "print(vocabulary)\n",
        "arr1=np.zeros(len(vocabulary))\n",
        "def vectorize(sentence):\n",
        "  z= clean_text_nltk(sentence)\n",
        "  for words in z:\n",
        "    print(1)\n",
        "    for i in range (0,len(vocabulary)):\n",
        "     if words.lower()==vocabulary[i]:\n",
        "      arr1[i]=+1\n",
        "  return arr1\n",
        "\n",
        "print(vectorize(\"the quick brown fox jumps over the lazy dog.\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yVUFCkm7yrg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d7bfba5-499d-4e89-d0ba-5f5fd33d4510"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['absolute', 'algebra', 'artificial', 'back', 'be', 'behind', 'brown', 'concern', 'data', 'deliciou', 'dog', 'ethical', 'fox', 'go', 'hate', 'however', 'i', 'intelligence', 'involve', 'jump', 'lazy', 'learn', 'linear', 'love', 'machine', 'math', 'mind', 'nobler', 'not', 'over', 'pizza', 'question', 'quick', 'remain', 'science', 'service', 'statistic', 'terrible', 'that', 'the', 'ti', 'transform', 'whether', 'wont', 'world']\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2: BoW Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Instantiate the vectorizer.\n",
        "\n",
        "2. fit_transform the raw corpus.\n",
        "\n",
        "3. Convert the result to an array (.toarray()) and print it."
      ],
      "metadata": {
        "id": "UwsoZix-zUDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer= CountVectorizer()\n",
        "x= vectorizer.fit_transform(corpus)\n",
        "v= x.toarray()\n",
        "df= pd.DataFrame( index=corpus, data=v, columns=vectorizer.get_feature_names_out())\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "RGs7EzLRzfGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b72ce5-049e-4db0-ef66-bffe76ed9ba9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    absolutely  algebra  and  \\\n",
            "Artificial Intelligence is transforming the wor...           0        0    0   \n",
            "The pizza was absolutely delicious, but the ser...           1        0    0   \n",
            "The quick brown fox jumps over the lazy dog.                 0        0    0   \n",
            "To be, or not to be, that is the question: Whet...           0        0    0   \n",
            "Data science involves statistics, linear algebr...           0        1    1   \n",
            "I love machine learning, but I hate the math be...           0        0    0   \n",
            "\n",
            "                                                    artificial  back  be  \\\n",
            "Artificial Intelligence is transforming the wor...           1     0   0   \n",
            "The pizza was absolutely delicious, but the ser...           0     1   0   \n",
            "The quick brown fox jumps over the lazy dog.                 0     0   0   \n",
            "To be, or not to be, that is the question: Whet...           0     0   2   \n",
            "Data science involves statistics, linear algebr...           0     0   0   \n",
            "I love machine learning, but I hate the math be...           0     0   0   \n",
            "\n",
            "                                                    behind  brown  but  \\\n",
            "Artificial Intelligence is transforming the wor...       0      0    0   \n",
            "The pizza was absolutely delicious, but the ser...       0      0    1   \n",
            "The quick brown fox jumps over the lazy dog.             0      1    0   \n",
            "To be, or not to be, that is the question: Whet...       0      0    0   \n",
            "Data science involves statistics, linear algebr...       0      0    0   \n",
            "I love machine learning, but I hate the math be...       1      0    1   \n",
            "\n",
            "                                                    concerns  ...  terrible  \\\n",
            "Artificial Intelligence is transforming the wor...         1  ...         0   \n",
            "The pizza was absolutely delicious, but the ser...         0  ...         1   \n",
            "The quick brown fox jumps over the lazy dog.               0  ...         0   \n",
            "To be, or not to be, that is the question: Whet...         0  ...         0   \n",
            "Data science involves statistics, linear algebr...         0  ...         0   \n",
            "I love machine learning, but I hate the math be...         0  ...         0   \n",
            "\n",
            "                                                    that  the  tis  to  \\\n",
            "Artificial Intelligence is transforming the wor...     0    1    0   0   \n",
            "The pizza was absolutely delicious, but the ser...     0    2    0   0   \n",
            "The quick brown fox jumps over the lazy dog.           0    2    0   0   \n",
            "To be, or not to be, that is the question: Whet...     1    2    1   2   \n",
            "Data science involves statistics, linear algebr...     0    0    0   0   \n",
            "I love machine learning, but I hate the math be...     0    1    0   0   \n",
            "\n",
            "                                                    transforming  was  \\\n",
            "Artificial Intelligence is transforming the wor...             1    0   \n",
            "The pizza was absolutely delicious, but the ser...             0    2   \n",
            "The quick brown fox jumps over the lazy dog.                   0    0   \n",
            "To be, or not to be, that is the question: Whet...             0    0   \n",
            "Data science involves statistics, linear algebr...             0    0   \n",
            "I love machine learning, but I hate the math be...             0    0   \n",
            "\n",
            "                                                    whether  won  world  \n",
            "Artificial Intelligence is transforming the wor...        0    0      1  \n",
            "The pizza was absolutely delicious, but the ser...        0    1      0  \n",
            "The quick brown fox jumps over the lazy dog.              0    0      0  \n",
            "To be, or not to be, that is the question: Whet...        1    0      0  \n",
            "Data science involves statistics, linear algebr...        0    0      0  \n",
            "I love machine learning, but I hate the math be...        0    0      0  \n",
            "\n",
            "[6 rows x 52 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3: TF-IDF From Scratch (The Math)**\n",
        "\n",
        "**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n",
        "\n",
        "\"I love machine learning, but I hate the math behind it.\"\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n",
        "\n",
        "*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n",
        "\n",
        "**Result:** TF * IDF.\n",
        "\n",
        "**Task:** Print your manual calculation result."
      ],
      "metadata": {
        "id": "-MR6Bxgh0Gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence= \"I love machine learning, but I hate the math behind it.\"\n",
        "import math\n",
        "tokens=clean_text_scratch(sentence)\n",
        "a=tokens.count(\"machine\")\n",
        "b=len(tokens)\n",
        "tf= a/b\n",
        "c= len(corpus)\n",
        "d=0\n",
        "for block in arr:\n",
        "  for word in block:\n",
        "    if word==\"machine\":\n",
        "      d=d+1\n",
        "idf = math.log(c/d)\n",
        "print(tf*idf)"
      ],
      "metadata": {
        "id": "gNSo-nza0k_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec85ef36-3643-4b78-e9a1-14b76532ad27"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12206803207423442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4: TF-IDF Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n",
        "\n",
        "**Steps:** Fit it on the corpus and print the vector for the first sentence.\n",
        "\n",
        "**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"
      ],
      "metadata": {
        "id": "YEYkuoSb0nDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer= TfidfVectorizer()\n",
        "x = vectorizer.fit_transform([corpus[0]])\n",
        "v=x.toarray()\n",
        "df = pd.DataFrame(data=v,columns=vectorizer.get_feature_names_out())\n",
        "print(df)"
      ],
      "metadata": {
        "id": "Of6PfWyd0pnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34365815-2798-4a55-fa4d-1598136a17a6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   artificial  concerns   ethical   however  intelligence        is    remain  \\\n",
            "0    0.316228  0.316228  0.316228  0.316228      0.316228  0.316228  0.316228   \n",
            "\n",
            "        the  transforming     world  \n",
            "0  0.316228      0.316228  0.316228  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3- Word Embeddings**"
      ],
      "metadata": {
        "id": "YWAar8IIzp_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1: Word2Vec Using Tools**\n",
        "\n",
        "**Task:** Train a model using gensim.models.Word2Vec.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n",
        "\n",
        "2. Set min_count=1 (since our corpus is small, we want to keep all words).\n",
        "\n",
        "3. Set vector_size=10 (small vector size for easy viewing).\n",
        "\n",
        "**Experiment:** Print the vector for the word \"learning\"."
      ],
      "metadata": {
        "id": "uY1URFxgz036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#rest of the code here"
      ],
      "metadata": {
        "id": "aziX2IGBzyaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1ea6cf3-c855-47e0-9130-4c311dec9dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3: Pre-trained GloVe (Understanding Global Context)**\n",
        "\n",
        "**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n",
        "\n",
        "**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n",
        "\n",
        "Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n",
        "\n",
        "**Question:** Does the model correctly guess \"Queen\"?"
      ],
      "metadata": {
        "id": "r3J42eQZ1fUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained GloVe model\n",
        "glove_model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "#rest of the code here"
      ],
      "metadata": {
        "id": "LEj5SkO81mkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d8f0767-516d-49f0-8681-80a580cf8fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5- Sentiment Analysis (The Application)**\n",
        "\n",
        "**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Initialize the SentimentIntensityAnalyzer.\n",
        "\n",
        "2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "\n",
        "3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "\n",
        "**Analysis:** Look at the compound score for both.\n",
        "\n",
        "**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n",
        "\n",
        "Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
      ],
      "metadata": {
        "id": "AbI4K0UJUxy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# Download VADER (run once)\n",
        "nltk.download('vader_lexicon')\n",
        "# rest of the code here"
      ],
      "metadata": {
        "id": "_lC2c3GHUxU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193bc48c-519d-4085-8336-b622ffd5bf36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}